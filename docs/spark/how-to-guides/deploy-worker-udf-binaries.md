---
title: 部署 .NET for Apache Spark 辅助角色和用户定义的函数二进制文件
description: 了解如何部署 .NET for Apache Spark 辅助角色和用户定义的函数二进制文件。
ms.date: 10/09/2020
ms.topic: conceptual
ms.custom: mvc,how-to
ms.openlocfilehash: d3e86d22a308c7124812f2c11742c2e18803b13d
ms.sourcegitcommit: c7f0beaa2bd66ebca86362ca17d673f7e8256ca6
ms.translationtype: HT
ms.contentlocale: zh-CN
ms.lasthandoff: 03/23/2021
ms.locfileid: "104875533"
---
# <a name="deploy-net-for-apache-spark-worker-and-user-defined-function-binaries"></a><span data-ttu-id="128bc-103">部署 .NET for Apache Spark 辅助角色和用户定义的函数二进制文件</span><span class="sxs-lookup"><span data-stu-id="128bc-103">Deploy .NET for Apache Spark worker and user-defined function binaries</span></span>

<span data-ttu-id="128bc-104">本操作说明提供有关如何部署 .NET for Apache Spark 辅助角色和用户定义的函数二进制文件的常规说明。</span><span class="sxs-lookup"><span data-stu-id="128bc-104">This how-to provides general instructions on how to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span> <span data-ttu-id="128bc-105">你将了解要设置的环境变量，还将了解通过 `spark-submit` 启动应用程序的一些常用参数。</span><span class="sxs-lookup"><span data-stu-id="128bc-105">You learn which Environment Variables to set up, as well as some commonly used parameters for launching applications with `spark-submit`.</span></span>

## <a name="configurations"></a><span data-ttu-id="128bc-106">配置</span><span class="sxs-lookup"><span data-stu-id="128bc-106">Configurations</span></span>

<span data-ttu-id="128bc-107">配置显示常规环境变量和参数设置，用于部署 .NET for Apache Spark 辅助角色和用户定义的函数二进制文件。</span><span class="sxs-lookup"><span data-stu-id="128bc-107">Configurations show the general environment variables and parameters settings in order to deploy .NET for Apache Spark worker and user-defined function binaries.</span></span>

### <a name="environment-variables"></a><span data-ttu-id="128bc-108">环境变量</span><span class="sxs-lookup"><span data-stu-id="128bc-108">Environment variables</span></span>

<span data-ttu-id="128bc-109">部署辅助角色和编写 UDF 时，可能需要设置几个常用环境变量：</span><span class="sxs-lookup"><span data-stu-id="128bc-109">When deploying workers and writing UDFs, there are a few commonly used environment variables that you may need to set:</span></span>

| <span data-ttu-id="128bc-110">环境变量</span><span class="sxs-lookup"><span data-stu-id="128bc-110">Environment Variable</span></span>         | <span data-ttu-id="128bc-111">描述</span><span class="sxs-lookup"><span data-stu-id="128bc-111">Description</span></span>
| :--------------------------- | :----------
| <span data-ttu-id="128bc-112">DOTNET_WORKER_DIR</span><span class="sxs-lookup"><span data-stu-id="128bc-112">DOTNET_WORKER_DIR</span></span>            | <span data-ttu-id="128bc-113">生成 <code>Microsoft.Spark.Worker</code> 二进制文件的路径</span><span class="sxs-lookup"><span data-stu-id="128bc-113">Path where the <code>Microsoft.Spark.Worker</code> binary has been generated.</span></span></br><span data-ttu-id="128bc-114">它由 Spark 驱动程序使用，将被传递到 Spark 执行程序。</span><span class="sxs-lookup"><span data-stu-id="128bc-114">It's used by the Spark driver and will be passed to Spark executors.</span></span> <span data-ttu-id="128bc-115">如果未设置此变量，Spark 执行器将搜索 <code>PATH</code> 环境变量中指定的路径。</span><span class="sxs-lookup"><span data-stu-id="128bc-115">If this variable is not set up, the Spark executors will search the path specified in the <code>PATH</code> environment variable.</span></span></br><span data-ttu-id="128bc-116">例如  “C:\bin\Microsoft.Spark.Worker”</span><span class="sxs-lookup"><span data-stu-id="128bc-116">_e.g. "C:\bin\Microsoft.Spark.Worker"_</span></span>
| <span data-ttu-id="128bc-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span><span class="sxs-lookup"><span data-stu-id="128bc-117">DOTNET_ASSEMBLY_SEARCH_PATHS</span></span> | <span data-ttu-id="128bc-118">逗号分隔的路径，<code>Microsoft.Spark.Worker</code> 将在这些路径中加载程序集。</span><span class="sxs-lookup"><span data-stu-id="128bc-118">Comma-separated paths where <code>Microsoft.Spark.Worker</code> will load assemblies.</span></span></br><span data-ttu-id="128bc-119">请注意，如果路径以“.”开头，则将预置工作目录。</span><span class="sxs-lookup"><span data-stu-id="128bc-119">Note that if a path starts with ".", the working directory will be prepended.</span></span> <span data-ttu-id="128bc-120">如果处于 yarn 模式，  则“.”表示容器的工作目录。</span><span class="sxs-lookup"><span data-stu-id="128bc-120">If in **yarn mode**, "." would represent the container's working directory.</span></span></br><span data-ttu-id="128bc-121">例如  “C:\Users\\&lt;用户名&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet 版本&gt;”</span><span class="sxs-lookup"><span data-stu-id="128bc-121">_e.g. "C:\Users\\&lt;user name&gt;\\&lt;mysparkapp&gt;\bin\Debug\\&lt;dotnet version&gt;"_</span></span>
| <span data-ttu-id="128bc-122">DOTNET_WORKER_DEBUG</span><span class="sxs-lookup"><span data-stu-id="128bc-122">DOTNET_WORKER_DEBUG</span></span>          | <span data-ttu-id="128bc-123">如果要<a href="https://github.com/dotnet/spark/blob/main/docs/developer-guide.md#debugging-user-defined-function-udf">调试 UDF</a>，请在运行 <code>spark-submit</code> 之前将此环境变量设置为 <code>1</code>。</span><span class="sxs-lookup"><span data-stu-id="128bc-123">If you want to <a href="https://github.com/dotnet/spark/blob/main/docs/developer-guide.md#debugging-user-defined-function-udf">debug a UDF</a>, then set this environment variable to <code>1</code> before running <code>spark-submit</code>.</span></span>

### <a name="parameter-options"></a><span data-ttu-id="128bc-124">参数选项</span><span class="sxs-lookup"><span data-stu-id="128bc-124">Parameter options</span></span>

<span data-ttu-id="128bc-125">[捆绑](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies) Spark 应用程序后，可以使用 `spark-submit` 启动它。</span><span class="sxs-lookup"><span data-stu-id="128bc-125">Once the Spark application is [bundled](https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies), you can launch it using `spark-submit`.</span></span> <span data-ttu-id="128bc-126">下表显示了一些常用选项：</span><span class="sxs-lookup"><span data-stu-id="128bc-126">The following table shows some of the commonly used options:</span></span>

| <span data-ttu-id="128bc-127">参数名称</span><span class="sxs-lookup"><span data-stu-id="128bc-127">Parameter Name</span></span>        | <span data-ttu-id="128bc-128">描述</span><span class="sxs-lookup"><span data-stu-id="128bc-128">Description</span></span>
| :---------------------| :----------
| <span data-ttu-id="128bc-129">--class</span><span class="sxs-lookup"><span data-stu-id="128bc-129">--class</span></span>               | <span data-ttu-id="128bc-130">应用程序的入口点。</span><span class="sxs-lookup"><span data-stu-id="128bc-130">The entry point for your application.</span></span></br><span data-ttu-id="128bc-131"> 例如 org.apache.spark.deploy.dotnet.DotnetRunner</span><span class="sxs-lookup"><span data-stu-id="128bc-131">_e.g. org.apache.spark.deploy.dotnet.DotnetRunner_</span></span>
| <span data-ttu-id="128bc-132">--master</span><span class="sxs-lookup"><span data-stu-id="128bc-132">--master</span></span>              | <span data-ttu-id="128bc-133">群集的<a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">母版 URL</a>。</span><span class="sxs-lookup"><span data-stu-id="128bc-133">The <a href="https://spark.apache.org/docs/latest/submitting-applications.html#master-urls">master URL</a> for the cluster.</span></span></br><span data-ttu-id="128bc-134"> 例如 yarn</span><span class="sxs-lookup"><span data-stu-id="128bc-134">_e.g. yarn_</span></span>
| <span data-ttu-id="128bc-135">--deploy-mode</span><span class="sxs-lookup"><span data-stu-id="128bc-135">--deploy-mode</span></span>         | <span data-ttu-id="128bc-136">是在辅助节点 (<code>cluster</code>) 上还是在本地作为外部客户端 (<code>client</code>) 部署驱动程序。</span><span class="sxs-lookup"><span data-stu-id="128bc-136">Whether to deploy your driver on the worker nodes (<code>cluster</code>) or locally as an external client (<code>client</code>).</span></span></br><span data-ttu-id="128bc-137">默认值：<code>client</code></span><span class="sxs-lookup"><span data-stu-id="128bc-137">Default: <code>client</code></span></span>
| <span data-ttu-id="128bc-138">--conf</span><span class="sxs-lookup"><span data-stu-id="128bc-138">--conf</span></span>                | <span data-ttu-id="128bc-139"><code>key=value</code> 格式的任意 Spark 配置属性。</span><span class="sxs-lookup"><span data-stu-id="128bc-139">Arbitrary Spark configuration property in <code>key=value</code> format.</span></span></br><span data-ttu-id="128bc-140"> 例如 spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker</span><span class="sxs-lookup"><span data-stu-id="128bc-140">_e.g. spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=.\worker\Microsoft.Spark.Worker_</span></span>
| <span data-ttu-id="128bc-141">--files</span><span class="sxs-lookup"><span data-stu-id="128bc-141">--files</span></span>               | <span data-ttu-id="128bc-142">要放入每个执行程序的工作目录中的文件的逗号分隔列表。</span><span class="sxs-lookup"><span data-stu-id="128bc-142">Comma-separated list of files to be placed in the working directory of each executor.</span></span><br/><ul><li><span data-ttu-id="128bc-143">请注意，此选项仅适用于 yarn 模式。</span><span class="sxs-lookup"><span data-stu-id="128bc-143">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="128bc-144">它支持通过 # 指定文件名，与 Hadoop 类似。</span><span class="sxs-lookup"><span data-stu-id="128bc-144">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="128bc-145">例如 <code>myLocalSparkApp.dll#appSeen.dll</code>。  在 YARN 上运行时，应用程序应使用如 <code>appSeen.dll</code> 的名称引用 <code>myLocalSparkApp.dll</code>。</span><span class="sxs-lookup"><span data-stu-id="128bc-145">_e.g. <code>myLocalSparkApp.dll#appSeen.dll</code>. Your application should use the name as <code>appSeen.dll</code> to reference <code>myLocalSparkApp.dll</code> when running on YARN._</span></span></li>
| <span data-ttu-id="128bc-146">--archives</span><span class="sxs-lookup"><span data-stu-id="128bc-146">--archives</span></span>          | <span data-ttu-id="128bc-147">要提取到每个执行程序的工作目录中的存档的逗号分隔列表。</span><span class="sxs-lookup"><span data-stu-id="128bc-147">Comma-separated list of archives to be extracted into the working directory of each executor.</span></span></br><ul><li><span data-ttu-id="128bc-148">请注意，此选项仅适用于 yarn 模式。</span><span class="sxs-lookup"><span data-stu-id="128bc-148">Please note that this option is only applicable for yarn mode.</span></span></li><li><span data-ttu-id="128bc-149">它支持通过 # 指定文件名，与 Hadoop 类似。</span><span class="sxs-lookup"><span data-stu-id="128bc-149">It supports specifying file names with # similar to Hadoop.</span></span></br></ul><span data-ttu-id="128bc-150">例如 <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>。  这会将 zip 文件复制并提取到 <code>worker</code> 文件夹。</span><span class="sxs-lookup"><span data-stu-id="128bc-150">_e.g. <code>hdfs://&lt;path to your worker file&gt;/Microsoft.Spark.Worker.zip#worker</code>. This will copy and extract the zip file to <code>worker</code> folder._</span></span></li>
| <span data-ttu-id="128bc-151">application-jar</span><span class="sxs-lookup"><span data-stu-id="128bc-151">application-jar</span></span>       | <span data-ttu-id="128bc-152">指向包含应用程序和所有依赖项的捆绑 jar 的路径。</span><span class="sxs-lookup"><span data-stu-id="128bc-152">Path to a bundled jar including your application and all dependencies.</span></span></br><span data-ttu-id="128bc-153">例如 hdfs://&lt;jar 路径&gt;/microsoft-spark-&lt;版本&gt;.jar </span><span class="sxs-lookup"><span data-stu-id="128bc-153">_e.g. hdfs://&lt;path to your jar&gt;/microsoft-spark-&lt;version&gt;.jar_</span></span>
| <span data-ttu-id="128bc-154">application-arguments</span><span class="sxs-lookup"><span data-stu-id="128bc-154">application-arguments</span></span> | <span data-ttu-id="128bc-155">传递给主类 main 方法的参数（如果有）。</span><span class="sxs-lookup"><span data-stu-id="128bc-155">Arguments passed to the main method of your main class, if any.</span></span></br><span data-ttu-id="128bc-156">例如 hdfs://&lt;应用路径&gt;/&lt;应用&gt;.zip &lt;应用名称&gt; &lt;应用参数&gt; </span><span class="sxs-lookup"><span data-stu-id="128bc-156">_e.g. hdfs://&lt;path to your app&gt;/&lt;your app&gt;.zip &lt;your app name&gt; &lt;app args&gt;_</span></span>

> [!NOTE]
> <span data-ttu-id="128bc-157">在 `application-jar` 通过 `spark-submit` 启动应用程序之前指定所有 `--options`，否则将忽略它们。</span><span class="sxs-lookup"><span data-stu-id="128bc-157">Specify all the `--options` before `application-jar` when launching applications with `spark-submit`, otherwise they will be ignored.</span></span> <span data-ttu-id="128bc-158">有关详细信息，请参阅 [`spark-submit` 选项](https://spark.apache.org/docs/latest/submitting-applications.html)和[在 YARN 上运行 Spark 详解](https://spark.apache.org/docs/latest/running-on-yarn.html)。</span><span class="sxs-lookup"><span data-stu-id="128bc-158">For more information, see [`spark-submit` options](https://spark.apache.org/docs/latest/submitting-applications.html) and [running spark on YARN details](https://spark.apache.org/docs/latest/running-on-yarn.html).</span></span>

## <a name="frequently-asked-questions"></a><span data-ttu-id="128bc-159">常见问题</span><span class="sxs-lookup"><span data-stu-id="128bc-159">Frequently asked questions</span></span>

### <a name="when-i-run-a-spark-app-with-udfs-i-get-a-filenotfoundexception-error-what-should-i-do"></a><span data-ttu-id="128bc-160">通过 UDF 运行 Spark 应用时，出现“FileNotFoundException”错误。</span><span class="sxs-lookup"><span data-stu-id="128bc-160">When I run a spark app with UDFs, I get a \`FileNotFoundException' error.</span></span> <span data-ttu-id="128bc-161">应采取何种操作？</span><span class="sxs-lookup"><span data-stu-id="128bc-161">What should I do?</span></span>

> <span data-ttu-id="128bc-162"> 错误：[Error] [TaskRunner] [0] ProcessStream() 失败，出现异常：System.IO.FileNotFoundException:Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' 找不到文件: 'mySparkApp.dll'</span><span class="sxs-lookup"><span data-stu-id="128bc-162">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.IO.FileNotFoundException: Assembly 'mySparkApp, Version=1.0.0.0, Culture=neutral, PublicKeyToken=null' file not found: 'mySparkApp.dll'</span></span>

<span data-ttu-id="128bc-163">**答：** 检查是否已正确设置 `DOTNET_ASSEMBLY_SEARCH_PATHS` 环境变量。</span><span class="sxs-lookup"><span data-stu-id="128bc-163">**Answer:** Check that the `DOTNET_ASSEMBLY_SEARCH_PATHS` environment variable is set correctly.</span></span> <span data-ttu-id="128bc-164">它应是包含 `mySparkApp.dll` 的路径。</span><span class="sxs-lookup"><span data-stu-id="128bc-164">It should be the path that contains your `mySparkApp.dll`.</span></span>

### <a name="after-i-upgraded-my-net-for-apache-spark-version-and-reset-the-dotnet_worker_dir-environment-variable-why-do-i-still-get-the-following-ioexception-error"></a><span data-ttu-id="128bc-165">升级 .NET for Apache Spark 版本并重置 `DOTNET_WORKER_DIR` 环境变量后，为什么仍会出现以下 `IOException` 错误？</span><span class="sxs-lookup"><span data-stu-id="128bc-165">After I upgraded my .NET for Apache Spark version and reset the `DOTNET_WORKER_DIR` environment variable, why do I still get the following `IOException` error?</span></span>

> <span data-ttu-id="128bc-166">错误：  在阶段 11.0 丢失任务0.0（TID 24、localhost、执行程序驱动程序）：java.io.IOException:无法运行程序 "Microsoft.Spark.Worker.exe":CreateProcess error=2，系统找不到指定的文件。</span><span class="sxs-lookup"><span data-stu-id="128bc-166">**Error:** Lost task 0.0 in stage 11.0 (TID 24, localhost, executor driver): java.io.IOException: Cannot run program "Microsoft.Spark.Worker.exe": CreateProcess error=2, The system cannot find the file specified.</span></span>

<span data-ttu-id="128bc-167">**答：** 首先尝试重启 PowerShell 窗口（或其他命令窗口），使其可以采用最新的环境变量值。</span><span class="sxs-lookup"><span data-stu-id="128bc-167">**Answer:** Try restarting your PowerShell window (or other command windows) first so that it can take the latest environment variable values.</span></span> <span data-ttu-id="128bc-168">然后启动程序。</span><span class="sxs-lookup"><span data-stu-id="128bc-168">Then start your program.</span></span>

### <a name="after-submitting-my-spark-application-i-get-the-error-systemtypeloadexception-could-not-load-type-systemruntimeremotingcontextscontext"></a><span data-ttu-id="128bc-169">提交 Spark 应用程序后，出现 `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'` 错误。</span><span class="sxs-lookup"><span data-stu-id="128bc-169">After submitting my Spark application, I get the error `System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context'`.</span></span>

> <span data-ttu-id="128bc-170"> 错误：[Error] [TaskRunner] [0] ProcessStream() 失败，出现异常：System.TypeLoadException 异常:未能从程序集 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...' 加载类型 'System.Runtime.Remoting.Contexts.Context'。</span><span class="sxs-lookup"><span data-stu-id="128bc-170">**Error:** [Error] [TaskRunner] [0] ProcessStream() failed with exception: System.TypeLoadException: Could not load type 'System.Runtime.Remoting.Contexts.Context' from assembly 'mscorlib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=...'.</span></span>

<span data-ttu-id="128bc-171">**答：** 检查你使用的 `Microsoft.Spark.Worker` 版本。</span><span class="sxs-lookup"><span data-stu-id="128bc-171">**Answer:** Check the `Microsoft.Spark.Worker` version you are using.</span></span> <span data-ttu-id="128bc-172">有两个版本：.NET Framework 4.6.1 和 .NET Core 3.1.x 。</span><span class="sxs-lookup"><span data-stu-id="128bc-172">There are two versions: **.NET Framework 4.6.1** and **.NET Core 3.1.x**.</span></span> <span data-ttu-id="128bc-173">在本例中，应使用 `Microsoft.Spark.Worker.net461.win-x64-<version>`（可以[下载](https://github.com/dotnet/spark/releases)），因为 `System.Runtime.Remoting.Contexts.Context` 仅适用于 .NET Framework。</span><span class="sxs-lookup"><span data-stu-id="128bc-173">In this case, `Microsoft.Spark.Worker.net461.win-x64-<version>` (which you can [download](https://github.com/dotnet/spark/releases)) should be used since `System.Runtime.Remoting.Contexts.Context` is only for .NET Framework.</span></span>

### <a name="how-do-i-run-my-spark-application-with-udfs-on-yarn-which-environment-variables-and-parameters-should-i-use"></a><span data-ttu-id="128bc-174">如何在 YARN 上通过 UDF 运行 Spark 应用程序？</span><span class="sxs-lookup"><span data-stu-id="128bc-174">How do I run my spark application with UDFs on YARN?</span></span> <span data-ttu-id="128bc-175">应该使用哪些环境变量和参数？</span><span class="sxs-lookup"><span data-stu-id="128bc-175">Which environment variables and parameters should I use?</span></span>

<span data-ttu-id="128bc-176">**答：** 若要在 YARN 上启动 Spark 应用程序，应将环境变量指定为 `spark.yarn.appMasterEnv.[EnvironmentVariableName]`。</span><span class="sxs-lookup"><span data-stu-id="128bc-176">**Answer:** To launch the spark application on YARN, the environment variables should be specified as `spark.yarn.appMasterEnv.[EnvironmentVariableName]`.</span></span> <span data-ttu-id="128bc-177">请参阅以下内容，作为使用 `spark-submit` 的示例：</span><span class="sxs-lookup"><span data-stu-id="128bc-177">Please see below as an example using `spark-submit`:</span></span>

```powershell
spark-submit \
--class org.apache.spark.deploy.dotnet.DotnetRunner \
--master yarn \
--deploy-mode cluster \
--conf spark.yarn.appMasterEnv.DOTNET_WORKER_DIR=./worker/Microsoft.Spark.Worker-<version> \
--conf spark.yarn.appMasterEnv.DOTNET_ASSEMBLY_SEARCH_PATHS=./udfs \
--archives hdfs://<path to your files>/Microsoft.Spark.Worker.net461.win-x64-<version>.zip#worker,hdfs://<path to your files>/mySparkApp.zip#udfs \
hdfs://<path to jar file>/microsoft-spark-<spark_majorversion-spark_minorversion>_<scala_majorversion.scala_minorversion>-<spark_dotnet_version>.jar \
hdfs://<path to your files>/mySparkApp.zip mySparkApp
```

## <a name="next-steps"></a><span data-ttu-id="128bc-178">后续步骤</span><span class="sxs-lookup"><span data-stu-id="128bc-178">Next steps</span></span>

* [<span data-ttu-id="128bc-179">.NET for Apache Spark 入门</span><span class="sxs-lookup"><span data-stu-id="128bc-179">Get started with .NET for Apache Spark</span></span>](../tutorials/get-started.md)
* [<span data-ttu-id="128bc-180">在 Windows 上调试 .NET for Apache Spark 应用程序</span><span class="sxs-lookup"><span data-stu-id="128bc-180">Debug a .NET for Apache Spark application on Windows</span></span>](debug.md)
